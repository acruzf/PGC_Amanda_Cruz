# -*- coding: utf-8 -*-
"""PGC_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PluqxXSpP5R36yNz8ivjW7kMvwqIQPIP

# **Imports**
"""

#nltk.download('wordnet')
!pip install unidecode
#!pip install googletrans
!pip install googletrans==3.1.0a0

#Libs

import csv
import pandas as pd
import csv
import string
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
import unidecode
from unidecode import unidecode
import networkx as nx
from networkx.algorithms import bipartite
import matplotlib.pyplot as plt



#import googletrans
#from googletrans import Translator
#from nltk.corpus import words

#Configurações

pd.options.display.max_colwidth = 250
pd.set_option('display.max_columns', None)
lemmatizer = WordNetLemmatizer()
stemmer = SnowballStemmer('portuguese')

"""# **Capturando dados**"""

#import dos dados alunos

!gdown --id 1F_9vsXfWh3-YjASHVKR4R1i1nHZ7MgxI

projetos = pd.read_csv('/content/projetos.csv', delimiter=';')
projetos.columns

#tratamento dos dados alunos

projetos.drop(projetos.tail(3).index,inplace=True)
print(projetos[['autor', 'palavras_chave']])

#import dos dados professores 1

!gdown --id 1l1xwKOCC-AT6jWXCOAF_FcCKO0ZvBuVH

avaliadores_1 = pd.read_csv('/content/Publicacoes_Capitulos_livros_v2.csv', delimiter=';')

avaliadores_1["palavras"]= avaliadores_1['Título']+" "+ avaliadores_1['Título-do-livro']
#avaliadores_2=avaliadores_1.groupby('ID-Lattes')['palavras'].apply(lambda x: ','.join(x)).reset_index()
avaliadores_2=avaliadores_1.groupby('Nome')['palavras'].apply(lambda x: ','.join(x)).reset_index()

avaliadores_2.head
avaliadores_2.loc[avaliadores_2['Nome'] == 'Walter Hugo Lopez Pinaya']

#import dos dados professores 2

!gdown --id 1bYm541eTqFpAHXqqow_NKrlSEHn614hF
avaliadores_1b = pd.read_csv('/content/artigos_periodicos.csv', delimiter=';')

avaliadores_1b["palavras2"]= avaliadores_1b['Título']+" "+ avaliadores_1b['Palavras-chave']
#avaliadores_2b=avaliadores_1b.groupby('ID-Lattes')['palavras2'].apply(lambda x: ','.join(x)).reset_index()
avaliadores_2b=avaliadores_1b.groupby('Nome')['palavras2'].apply(lambda x: ','.join(x)).reset_index()


#avaliadores_2b.head
avaliadores_2b.loc[avaliadores_2b['Nome'] == 'Walter Hugo Lopez Pinaya']

#tratamento dados professores

avaliadores_fin= pd.merge(avaliadores_2, avaliadores_2b, how = 'outer')
#avaliadores_fin["palavras3"]=  (avaliadores_fin['palavras2'])
#str(avaliadores_fin['palavras'])+" "+
avaliadores_fin["palavras3"]= avaliadores_fin['palavras'].fillna('') + avaliadores_fin['palavras2'].fillna('')
#avaliadores_fin.head
avaliadores_fin.loc[avaliadores_fin['Nome'] == 'Vladimir Perchine']

"""# **Tratamento PLN**"""

#criação banco de stopwords

stopwords2=[]

stopwords = ["a","agora","ainda","alguém","algum","alguma","algumas","alguns","ampla","amplas","amplo","amplos","ante","antes","ao","aos","após","aquela","aquelas","aquele","aqueles","aquilo","as","até","através","cada","coisa","coisas","com","como","contra","contudo","da","daquele","daqueles","das","de","dela","delas","dele","deles","depois","dessa","dessas","desse","desses","desta","destas","deste","deste","destes","deve","devem","devendo","dever","deverá","deverão",
"deveria","deveriam","devia","deviam","disse","disso","disto","dito","diz","dizem","do","dos","e","é","ela","elas","ele","eles","em","enquanto","entre","era","essa","essas","esse","esses","esta","está","estamos","estão","estas",
"estava","estavam","estávamos","este","estes","estou","eu","fazendo","fazer","feita","feitas","feito","feitos","foi","for","foram","fosse","fossem","grande","grandes","há","isso","isto","já","la","lá","lhe","lhes","lo","mas","me","mesma","mesmas","mesmo","mesmos","meu","meus","minha","minhas","muita","muitas","muito","muitos","na","não","nas","nem","nenhum","nessa","nessas","nesta","nestas","ninguém","no","nos","nós","nossa","nossas","nosso","nossos","num","numa","nunca","o","os","ou","outra","outras","outro","outros","para","pela","pelas","pelo","pelos","pequena","pequenas","pequeno","pequenos","per","perante","pode","pude","podendo","poder","poderia","poderiam","podia","podiam","pois","por","porém","porque","posso",
"pouca","poucas","pouco","poucos","primeiro","primeiros","própria","próprias","próprio","próprios","quais","qual","quando","quanto","quantos","que","quem","são","se","seja","sejam","sem","sempre","sendo","será","serão","seu","seus","si","sido","só","sob","sobre","sua","suas","talvez","também","tampouco","te","tem","tendo","tenha","ter","teu","teus","ti","tido","tinha","tinham",
"toda","todas","todavia","todo","todos","tu","tua","tuas","tudo","última","últimas","último","últimos","um","uma","umas","uns","vendo","ver","vez","vindo","vir","vos","vós"]

for i in stopwords:
  stopwords2.append(stemmer.stem(unidecode(i)))

print(stopwords)
print(stopwords2)

#criação banco de palavras frequentes/ sem significado

lista_frequentes = ["algoritm",
"alternativ",
"amostr",
"analis",
"avaliaca",
"caracterist",
"caracterizaca",
"cienc",
"classificaca",
"colaborativ",
"conhecim",
"desempenh",
"desenvolv",
"disciplin",
"ensin",
"ensino",
"equaca",
"estrutur",
"experimental",
"fundament",
"fundamental",
"implantaca",
"importanc",
"influenc",
"informaca",
"linguagens",
"metod",
"metodolog",
"model",
"objet",
"paramet",
"performanc",
"period",
"perspectiv",
"principi",
"problem",
"produca",
"relaca",
"revisa",
"sistem",
"soluca",
"tecnolog",
"topic",
"trabalh",
"utiliz",
"utilizaca"]

palavras_retirar= lista_frequentes+stopwords+stopwords2

print(palavras_retirar)

"""# **Criação do grafo**"""

#criando nós projetos

aux_palavras=""

# lista de autores - nome dos nós
autores=projetos['autor'].values.tolist()

bag=projetos['palavras_chave'].values.tolist()
bag_pc=[]
for i in bag:
  aux_palavras=""
  for j in i.split(): 
    palavra=j.translate(str.maketrans('', '', string.punctuation))
    palavra2=stemmer.stem(unidecode(palavra))
    if (not(palavra2 in palavras_retirar) and len(palavra2)>1):
      aux_palavras= aux_palavras+" "+palavra2
  bag_pc.append(aux_palavras)


bag2=projetos['titulo'].values.tolist()
bag_tit=[]
for i in bag2:
  aux_palavras=""
  for j in i.split(): 
    palavra=j.translate(str.maketrans('', '', string.punctuation))
    palavra2=stemmer.stem(unidecode(palavra))
    if (not(palavra2 in palavras_retirar) and len(palavra2)>1):
      aux_palavras= aux_palavras+" "+palavra2
  bag_tit.append(aux_palavras)


bag3=projetos['resumo'].values.tolist()
bag_res=[]
for i in bag3:
  aux_palavras=""

  for j in i.split(): 
    palavra=j.translate(str.maketrans('', '', string.punctuation))
    palavra2=stemmer.stem(unidecode(palavra))
    if (not(palavra2 in palavras_retirar) and len(palavra2)>1):
      aux_palavras= aux_palavras+" "+palavra2
  bag_res.append(aux_palavras)

G = nx.Graph()
for i,j,k,l in zip(autores, bag_pc,bag_tit,bag_res):
  G.add_node(i, atribut=j+' '+k+' '+l,bipartite=0) 
  
print(G.nodes)
print(G.nodes(data=True))

#criando nós avaliadores

aux_palavras2=""

# lista de autores - nome dos nós
avaliadores=avaliadores_fin['Nome'].values.tolist()

#atributos
bag_aval=avaliadores_fin['palavras3'].values.tolist()
bag_pal=[]
for i in bag_aval:
  aux_palavras2=""
  for j in i.split(): 
    palavra=j.translate(str.maketrans('', '', string.punctuation))
    palavra2=stemmer.stem(unidecode(palavra))
    if (not(palavra2 in palavras_retirar) and len(palavra2)>1):
      aux_palavras2= aux_palavras2+" "+palavra2
  bag_pal.append(aux_palavras2)

for i,j in zip(avaliadores, bag_pal):
  G.add_node(i, atribut=j,bipartite=1,projetos=0) 

print(G.nodes(data=True))
#print(bag_pal)

#criando arestas
peso=0


for i in G.nodes: #projetos
  if (nx.get_node_attributes(G, "bipartite")[i]==0):
    for j in G.nodes : #avaliadores
       if (nx.get_node_attributes(G, "bipartite")[j]==1):
        aux1=set((nx.get_node_attributes(G, "atribut")[i]).split(" "))
        aux2=set((nx.get_node_attributes(G, "atribut")[j]).split(" "))
        peso=round(len(aux1.intersection(aux2))/len(aux1.union(aux2)),4)
        if peso>0 and i!=j:
          G.add_edge(i, j, weight=peso)

print(G.edges(data=True))

#adicionando informação nas arestas

for u,v,a in G.edges(data=True):
  aux1=set((nx.get_node_attributes(G, "atribut")[u]).split(" "))
  aux2=set((nx.get_node_attributes(G, "atribut")[v]).split(" "))
  G[u][v]["intersecao"] = ' '.join(aux1.intersection(aux2))

#removendo nós isolados

G.remove_nodes_from(list(nx.isolates(G)))

#garantindo que o grafo é bipartido
bipartite.is_bipartite(G)

nx.write_gexf(G, "test.gexf")

"""# **Emparelhamento**"""

#refazendo emparelhamento k vez, cada professor pode avaliar x projetos
k=3
x=3

emp=nx.max_weight_matching(G)
for l in emp:
    aux1=set((nx.get_node_attributes(G, "atribut")[l[0]]).split(" "))
    aux2=set((nx.get_node_attributes(G, "atribut")[l[1]]).split(" "))
    print(l,"peso=",round(len(aux1.intersection(aux2))/len(aux1.union(aux2)),4) ,aux1.intersection(aux2))
print("\n")

for i in range(k-1):
  for j in emp:
    G.remove_edge(j[0], j[1]) #removendo arestas do ultimo emparelhamento

    if (nx.get_node_attributes(G, "bipartite")[j[0]]==1): 
      nx.set_node_attributes(G, {j[0]:nx.get_node_attributes(G, "projetos")[j[0]]+1}, name="projetos")
    if (nx.get_node_attributes(G, "bipartite")[j[1]]==1):
      nx.set_node_attributes(G, {j[1]:nx.get_node_attributes(G, "projetos")[j[1]]+1}, name="projetos")

    #removendo avaliador caso já tenha atingido x projetos
    if (nx.get_node_attributes(G, "bipartite")[j[0]]==1 and nx.get_node_attributes(G, "projetos")[j[0]]==x): 
      G.remove_node(j[0])
    if (nx.get_node_attributes(G, "bipartite")[j[1]]==1 and nx.get_node_attributes(G, "projetos")[j[1]]==x):
      G.remove_node(j[1])

  emp=nx.max_weight_matching(G) #refazendo o emparelhamento
  for l in emp:
    aux1=set((nx.get_node_attributes(G, "atribut")[l[0]]).split(" "))
    aux2=set((nx.get_node_attributes(G, "atribut")[l[1]]).split(" "))
    print(l,"peso=",round(len(aux1.intersection(aux2))/len(aux1.union(aux2)),4) ,aux1.intersection(aux2))
  
  print("\n")